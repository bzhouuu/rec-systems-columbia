{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Reviews Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bangwei Zhou (bz2280) Github: bzhouuu<br>\n",
    "James Jungsuk Lee (jl5241) Github: yjs1210<br>\n",
    "Ujjwal Peshin (up2138) Github: ujjwal95<br>\n",
    "Zhongling Jiang (zj2249) Github: jiangzl2016<br>\n",
    "\n",
    "## A. Abstract\n",
    "\n",
    "We compare the merits of n different recommendation algorithms side by side.\n",
    "\n",
    "* Collaborative Filtering (CF - Baseline)\n",
    "* Collective Matrix Factorization (CMF)\n",
    "* Content-Based Filtering (CBF)\n",
    "* Field-aware Factorization Machine (FFM) \n",
    "* Deep Learning Model\n",
    "\n",
    "* Hybrid Approach <br>\n",
    "\n",
    "[RESULTS]\n",
    "\n",
    "[CONCLUSION AND RECOMMENDATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Business Case and Objective\n",
    "\n",
    "Our goal through this exercise is to figure out the best methodology for recommending restaurants to our users. This will be largely measured by using RMSE, which measures our forecast’s error relative to the ground truth as well as ranking metrics. Additionally, we want to measure metrics that can help us assess the sanity of our recommendation and experiences of the users. We measure these using coverage and novelty metrics to ensure that there is enough diversity in our recommendations and that our model isn’t resorting to recommending the most popular restaurants as opposed to creating a truly personalized experience. \n",
    " \n",
    "There are additional considerations such as model computation time, which affects long term overhead of maintenance of the product, and model comprehension. These are important considerations for the business as inefficient solutions will add technology debt and burden to the company’s systems. Also, employing models that are well understood can help the team in getting buy-in from stakeholders. Therefore, these factors will be critically analyzed when assessing the merits of our algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data\n",
    "\n",
    "We use the publicly available yelp dataset which consists roughly of 7 million reviews from 1.6 million users across 190 thousand businesses in 10 metropolitan areas. We are additionally provided with business and user metadata as well as check-in information, tips and photos provided by the reviewers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = tarfile.open('yelp_dataset.tar') \n",
    "zf.extract('review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6685900/6685900 [01:04<00:00, 104276.79it/s]\n"
     ]
    }
   ],
   "source": [
    "line_count = len(open(\"review.json\").readlines())\n",
    "user_ids, business_ids, stars, dates, texts = [], [], [], [], []\n",
    "with open(\"review.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        user_ids += [blob[\"user_id\"]]\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        dates += [blob[\"date\"]]\n",
    "        texts += [blob[\"text\"]]\n",
    "ratings_ = pd.DataFrame(\n",
    "    {\"user_id\": user_ids, \"business_id\": business_ids, \"rating\": stars, \"date\": dates, \"text\": texts}\n",
    ")\n",
    "user_counts = ratings_[\"user_id\"].value_counts()\n",
    "active_users = user_counts.loc[user_counts >= 5].index.tolist()\n",
    "ratings_ = ratings_.loc[ratings_.user_id.isin(active_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3fw2X5bZYeW9xCz_zGhOHg</td>\n",
       "      <td>2016-05-07 01:21:02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Tracy dessert had a big name in Hong Kong and ...</td>\n",
       "      <td>jlu4CztcSxrKx56ba1a5AQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zvO-PJCpNk4fgAVUnExYAA</td>\n",
       "      <td>2010-10-05 19:12:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This place has gone down hill.  Clearly they h...</td>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b2jN2mm9Wf3RcrZCgfo1cg</td>\n",
       "      <td>2015-01-18 14:04:18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I was really looking forward to visiting after...</td>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 date  rating  \\\n",
       "0  ujmEBvifdJM6h6RLv4wQIg  2013-05-07 04:34:36     1.0   \n",
       "2  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
       "6  3fw2X5bZYeW9xCz_zGhOHg  2016-05-07 01:21:02     3.0   \n",
       "7  zvO-PJCpNk4fgAVUnExYAA  2010-10-05 19:12:35     1.0   \n",
       "8  b2jN2mm9Wf3RcrZCgfo1cg  2015-01-18 14:04:18     2.0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  Total bill for this horrible service? Over $8G...  hG7b0MtEbXx5QzbzE6C_VA  \n",
       "2  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "6  Tracy dessert had a big name in Hong Kong and ...  jlu4CztcSxrKx56ba1a5AQ  \n",
       "7  This place has gone down hill.  Clearly they h...  d6xvYpyzcfbF_AZ8vMB7QA  \n",
       "8  I was really looking forward to visiting after...  sG_h0dIzTKWa3Q6fmb4u-g  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users: 286130, unique restaurants: 185723\n"
     ]
    }
   ],
   "source": [
    "n_users = len(ratings_.user_id.unique())\n",
    "n_restaurants = len(ratings_.business_id.unique())\n",
    "print('Unique Users: {0}, unique restaurants: {1}'.format(n_users, n_restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in User attributes set and Restaurant attributes set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ = pd.read_csv('active_users.csv')\n",
    "business_ = pd.read_csv('businesses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>user_yelping_since</th>\n",
       "      <th>friends</th>\n",
       "      <th>useful_reviews</th>\n",
       "      <th>funny_reviews</th>\n",
       "      <th>cool_reviews</th>\n",
       "      <th>n_fans</th>\n",
       "      <th>years_elite</th>\n",
       "      <th>average_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l6BmjZMeQD3rDxWUbiAiow</td>\n",
       "      <td>Rashmi</td>\n",
       "      <td>95</td>\n",
       "      <td>2013-10-08 23:11:33</td>\n",
       "      <td>c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g...</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2015,2016,2017</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4XChL029mKr5hydo79Ljxg</td>\n",
       "      <td>Jenna</td>\n",
       "      <td>33</td>\n",
       "      <td>2013-02-21 22:29:06</td>\n",
       "      <td>kEBTgDvFX754S68FllfCaA, aB2DynOxNOJK9st2ZeGTPg...</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bc8C_eETBWL0olvFSJJd0w</td>\n",
       "      <td>David</td>\n",
       "      <td>16</td>\n",
       "      <td>2013-10-04 00:16:10</td>\n",
       "      <td>4N-HU_T32hLENLntsNKNBg, pSY2vwWLgWfGVAAiKQzMng...</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MM4RJAeH6yuaN8oZDSt0RA</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>361</td>\n",
       "      <td>2013-10-23 07:02:50</td>\n",
       "      <td>mbwrZ-RS76V1HoJ0bF_Geg, g64lOV39xSLRZO0aQQ6DeQ...</td>\n",
       "      <td>1114</td>\n",
       "      <td>279</td>\n",
       "      <td>665</td>\n",
       "      <td>39</td>\n",
       "      <td>2015,2016,2017,2018</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEtzbpgA2BFBrC0y0sCbfw</td>\n",
       "      <td>Keane</td>\n",
       "      <td>1122</td>\n",
       "      <td>2006-02-15 18:29:35</td>\n",
       "      <td>RJQTcJVlBsJ3_Yo0JSFQQg, GWt_h78k1CBBkE1NpThGfQ...</td>\n",
       "      <td>13311</td>\n",
       "      <td>19356</td>\n",
       "      <td>15319</td>\n",
       "      <td>696</td>\n",
       "      <td>2006,2007,2008,2009,2010,2011,2012,2013</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id user_name  user_review_count   user_yelping_since  \\\n",
       "0  l6BmjZMeQD3rDxWUbiAiow    Rashmi                 95  2013-10-08 23:11:33   \n",
       "1  4XChL029mKr5hydo79Ljxg     Jenna                 33  2013-02-21 22:29:06   \n",
       "2  bc8C_eETBWL0olvFSJJd0w     David                 16  2013-10-04 00:16:10   \n",
       "3  MM4RJAeH6yuaN8oZDSt0RA     Nancy                361  2013-10-23 07:02:50   \n",
       "4  TEtzbpgA2BFBrC0y0sCbfw     Keane               1122  2006-02-15 18:29:35   \n",
       "\n",
       "                                             friends  useful_reviews  \\\n",
       "0  c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g...              84   \n",
       "1  kEBTgDvFX754S68FllfCaA, aB2DynOxNOJK9st2ZeGTPg...              48   \n",
       "2  4N-HU_T32hLENLntsNKNBg, pSY2vwWLgWfGVAAiKQzMng...              28   \n",
       "3  mbwrZ-RS76V1HoJ0bF_Geg, g64lOV39xSLRZO0aQQ6DeQ...            1114   \n",
       "4  RJQTcJVlBsJ3_Yo0JSFQQg, GWt_h78k1CBBkE1NpThGfQ...           13311   \n",
       "\n",
       "   funny_reviews  cool_reviews  n_fans  \\\n",
       "0             17            25       5   \n",
       "1             22            16       4   \n",
       "2              8            10       0   \n",
       "3            279           665      39   \n",
       "4          19356         15319     696   \n",
       "\n",
       "                               years_elite  average_stars  \n",
       "0                           2015,2016,2017           4.03  \n",
       "1                                      NaN           3.63  \n",
       "2                                      NaN           3.71  \n",
       "3                      2015,2016,2017,2018           4.08  \n",
       "4  2006,2007,2008,2009,2010,2011,2012,2013           4.39  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_name</th>\n",
       "      <th>business_address</th>\n",
       "      <th>business_city</th>\n",
       "      <th>business_state</th>\n",
       "      <th>business_latitude</th>\n",
       "      <th>business_longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_counts</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
       "      <td>Arizona Biltmore Golf Club</td>\n",
       "      <td>2818 E Camino Acequia Drive</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.522143</td>\n",
       "      <td>-112.018481</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Golf, Active Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>ON</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>2.5</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
       "      <td>Farmers Insurance - Paul Lorenz</td>\n",
       "      <td>15655 W Roosevelt St, Ste 237</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.455613</td>\n",
       "      <td>-112.395596</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Insurance, Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
       "      <td>Queen City Plumbing</td>\n",
       "      <td>4209 Stuart Andrew Blvd, Ste F</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>35.190012</td>\n",
       "      <td>-80.887223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Plumbing, Shopping, Local Services, Home Servi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                    business_name  \\\n",
       "0  1SWheh84yJXfytovILXOAQ       Arizona Biltmore Golf Club   \n",
       "1  QXAEGFB4oINsVuTFxEYKFQ       Emerald Chinese Restaurant   \n",
       "2  gnKjwL_1w79qoiV3IC_xQQ      Musashi Japanese Restaurant   \n",
       "3  xvX2CttrVhyG2z1dFg_0xw  Farmers Insurance - Paul Lorenz   \n",
       "4  HhyxOkGAM07SRYtlQ4wMFQ              Queen City Plumbing   \n",
       "\n",
       "                 business_address business_city business_state  \\\n",
       "0     2818 E Camino Acequia Drive       Phoenix             AZ   \n",
       "1            30 Eglinton Avenue W   Mississauga             ON   \n",
       "2       10110 Johnston Rd, Ste 15     Charlotte             NC   \n",
       "3   15655 W Roosevelt St, Ste 237      Goodyear             AZ   \n",
       "4  4209 Stuart Andrew Blvd, Ste F     Charlotte             NC   \n",
       "\n",
       "   business_latitude  business_longitude  stars  review_counts  is_open  \\\n",
       "0          33.522143         -112.018481    3.0              5        0   \n",
       "1          43.605499          -79.652289    2.5            128        1   \n",
       "2          35.092564          -80.859132    4.0            170        1   \n",
       "3          33.455613         -112.395596    5.0              3        1   \n",
       "4          35.190012          -80.887223    4.0              4        1   \n",
       "\n",
       "                                          categories  \n",
       "0                                  Golf, Active Life  \n",
       "1  Specialty Food, Restaurants, Dim Sum, Imported...  \n",
       "2                  Sushi Bars, Restaurants, Japanese  \n",
       "3                      Insurance, Financial Services  \n",
       "4  Plumbing, Shopping, Local Services, Home Servi...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Subsampling and Train-Test Split\n",
    "#### E.1. FIltering, Undersampling and Dataset Size Considerations\n",
    "The idea behind undersampling is to develop a smaller dataset that is representable, or partially representative of the entire dataset, and expedites the model development cycle. We utilize 20% dataset size generally across the board for fast development iteration and for computation intensive tasks such as hyperparameter tuning. However, it is critical to analyze model performance across dataset sizes since the models’ capability to handle large dataset is an important consideration if it were to be utilized in production. Therefore we investigate two other dataset sizes as well, 50% and 100% of the data size. Whereas 20% of the dataset size is used generally, larger datasets are employed to analyze certain models’ capability to handle large data as well as measure how their performance changes with larger datasets. Finally, we filter our dataset to only active users who have at least 5 reviews since there needs to be at least some data about the user for the model to perform. If a user does not have at least 5 reviews, we will be building out recommendations using cold-start methods.\n",
    "\n",
    "#### E.2 Train Test Split\n",
    "\n",
    "Train-Test split is done in a very straightforward way. We take all the users that made through our filters as described above. Then we simply take the last two reviews of the users. The most recent review becomes our test set and the other second most recent review becomes our validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                 date  rating  \\\n",
      "0  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
      "1  hk5wpV-_pi5jmDDVPeG8DA  2018-09-14 18:50:19     5.0   \n",
      "2  30Q5xBagQHmkwp8Q9I1FCg  2018-02-03 23:27:43     5.0   \n",
      "3  UtWngqS-WloIY_A53W5K-Q  2016-02-18 06:42:16     5.0   \n",
      "4  dU-Nt1-LjV9mAgFOVcdAJw  2018-08-15 22:14:18     5.0   \n",
      "\n",
      "                                                text                 user_id  \n",
      "0  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "1  I highly recommend Arizona Pet Mortuary, David...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "2  First time at this restaurant our server \"Ramo...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "3  Such an amazing hospital with friendly staff, ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "4  Went for my yearly GYN exam and was seen by Lo...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "(918368, 5)\n"
     ]
    }
   ],
   "source": [
    "# Downsample by users\n",
    "user_id_unique = ratings_.user_id.unique()\n",
    "user_id_sample = pd.DataFrame(user_id_unique, columns=['unique_user_id']) \\\n",
    "                    .sample(frac= SAMPLING_RATE, replace=False, random_state=1)\n",
    "\n",
    "ratings_sample = ratings_.merge(user_id_sample, left_on='user_id', right_on='unique_user_id') \\\n",
    "                    .drop(['unique_user_id'], axis=1)\n",
    "print(ratings_sample.head())\n",
    "print(ratings_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 803897 rows, 5 columns in training set.\n",
      "There are 57229 rows, 5 columns in training set.\n",
      "There are 57223 rows, 5 columns in holdout set.\n"
     ]
    }
   ],
   "source": [
    "# hold out last review\n",
    "ratings_user_date = ratings_sample.loc[:, ['user_id', 'date']]\n",
    "ratings_user_date.date = pd.to_datetime(ratings_user_date.date)\n",
    "index_holdout = ratings_user_date.groupby(['user_id'], sort=False)['date'].transform(max) == ratings_user_date['date']\n",
    "ratings_holdout_ = ratings_sample[index_holdout]\n",
    "ratings_traincv_ = ratings_sample[~index_holdout]\n",
    "\n",
    "ratings_user_date = ratings_traincv_.loc[:, ['user_id', 'date']]\n",
    "index_holdout = ratings_user_date.groupby(['user_id'], sort=False)['date'].transform(max) == ratings_user_date['date']\n",
    "ratings_cv_ = ratings_traincv_[index_holdout]\n",
    "ratings_train_ = ratings_traincv_[~index_holdout]\n",
    "\n",
    "# remove the user that has fake reviews \n",
    "\n",
    "cv_users_del = set(ratings_cv_.user_id) - set(ratings_train_.user_id)\n",
    "holdout_users_del = set(ratings_holdout_.user_id) - set(ratings_train_.user_id)\n",
    "ratings_cv_ = ratings_cv_[~ratings_cv_.user_id.isin(cv_users_del)]\n",
    "ratings_holdout_ = ratings_holdout_[~ratings_holdout_.user_id.isin(holdout_users_del)]\n",
    "\n",
    "# ratings_cv_ = ratings_cv_[~ratings_cv_.user_id.isin(['HiT9sg9pvDiEVMFHJYihXg'])]\n",
    "# ratings_holdout_ = ratings_holdout_[~ratings_holdout_.user_id.isin(['HiT9sg9pvDiEVMFHJYihXg'])]\n",
    "\n",
    "print('There are {0} rows, {1} columns in training set.'.format(ratings_train_.shape[0], ratings_train_.shape[1]))\n",
    "print('There are {0} rows, {1} columns in training set.'.format(ratings_cv_.shape[0], ratings_cv_.shape[1]))\n",
    "print('There are {0} rows, {1} columns in holdout set.'.format(ratings_holdout_.shape[0], ratings_holdout_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57223\n"
     ]
    }
   ],
   "source": [
    "# check if we have a enough user sample size (> 50000)\n",
    "number_of_unique_users = len(ratings_train_.user_id.unique())\n",
    "print(number_of_unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Evaluation Metrics\n",
    "\n",
    "A list of evaluation metrics the team uses are:\n",
    "\n",
    "#### F.1. Regression Metrics\n",
    "Regression metrics measure We present four different regression metrics that are measured but we primarily use the RMSE. \n",
    "Root Mean Square Error (RMSE) calculates square rooted sum of square residuals of predictions. It measures numerical difference between all ground-truth ratings and actual ratings in test set.\n",
    "Mean Absolute Error (MAE) calculates sum of absolute residuals. It measures numerical difference between all ground-truth ratings and actual ratings in test set, and more robust to outliers.\n",
    "R-squared measures what percentage of variance in target that is explained by predictions. The higher the value, the better the predictions.\n",
    "\n",
    "#### F.2. Ranking Metrics\n",
    "We first make top 10 recommendations of restaurants for each user and see how relevant the rankings were. First we discuss the metrics then discuss a few caveats to the approach we took to measure them.\n",
    "Inclusion of Last Review in Top 10 Recommendations: We train our model on the training set and make top 10 recommendations to the users. We then look at the test set and see if the user’s latest review was included in that top 10 recommendations. We calculate the proportion of users who received such recommendations. In other words we have,\n",
    "\n",
    "$$\\frac{1}{n} * \\sum_j^n \\sum_{i \\in j's top 10}^{10}Rel(i)$$\n",
    "\n",
    "where a recommendation is relevant if it is the latest visited restaurant of the user and n is the total number of users measured. \n",
    "Average Ranking of Latest Restaurant: We train our model on the training set and make a prediction on every single business  and user combination. We then measure the average rankings on that prediction of the latest business that the user visited. In other words we have,\n",
    "\n",
    "$$\\frac{1}{n} * \\sum_j^n\\sum_i^m Rank(i)(1_{i=latest business of j})$$\n",
    "\n",
    "Where the indicator function is one if the restaurant is the last visited restaurant of user j, n is the total number of users and m is the total number of businesses.\n",
    "Some caveats of our approach is that instead of making a prediction on the entire business universe, we make predictions on the businesses that are in the same city as the business of the user’s last review. This makes sense since it would be futile to recommend a restaurant in Los Angeles to a user who is in New York City. This also allows us to reduce the computation time of our recommendation, which is another key advantage we want to have when we serve our model to the users. Additionally, we measure the above two metrics on a subsample of users as opposed to all the users to save computation time, and we also only take subsample of ratings that were positive ratings since we want to measure how good our recommendations are. \n",
    "#### F.3. Coverage\n",
    "We measure the coverage of our recommendations by looking at the proportion of our recommendations that are distinct. In other words, we measure, <br>\n",
    "\n",
    "$$\\frac{number of distinct businesses in all recommendations to the subsample} \n",
    "{number of all recommendations to the subsample}$$\n",
    "\n",
    " \n",
    "#### F.4. Novelty\n",
    "We measure the novelty of our recommendations by simply taking the proportion of businesses in our top ten recommendations that the user hasn’t been to. This is crucial since we don’t want our recommendations to be filled with restaurants that the user has already been to. We measure novelty as simply. \n",
    "\n",
    "#### F.5. Runtime \n",
    "We measure the runtime of the model’s training as well as its prediction time on validation set using Python’s time library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics on Coverage and Serendipity\n",
    "\n",
    "First subsample a group of users that we will measure these metrics from\n",
    "\n",
    "Methodology:\n",
    "    We sample 5 users from each city where the user made the latest review.\n",
    "    These cities must have at least 100 unique businesses\n",
    "    These users must also have made a postive review(above their historical average)to those restaurants.\n",
    "        1. We recommend 10 restaurants to each user\n",
    "        2. We see if their latest restaurant makes it into the top 10 list (Ranking Metric)\n",
    "        3. We see for those 10 x 5 recommendations, how many of them are distinct businesses (Coverage)\n",
    "        4. We see for those top 10 recommendations, how many of them are restaurants they have not visited (Serendipity)\n",
    "    \n",
    "    Additionally, we measure what our ranking was for the latest restaurant that the user visited(Ranking Metric\n",
    "    \n",
    "\n",
    "\n",
    "Criteria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "                  'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_holdout = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train_final = ratings_train.append(ratings_val)\n",
    "ratings_entire_df = ratings_train.append(ratings_val).append(ratings_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'date', 'rating', 'text', 'user_id', 'week_day', 'month',\n",
       "       'hour', 'user_name', 'user_review_count', 'user_yelping_since',\n",
       "       'friends', 'useful_reviews', 'funny_reviews', 'cool_reviews', 'n_fans',\n",
       "       'years_elite', 'average_stars', 'business_name', 'address', 'city',\n",
       "       'state', 'latitude', 'longitude', 'stars', 'review_counts', 'is_open',\n",
       "       'categories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_holdout.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_city_businesses = ratings_entire_df[['city','business_id']].drop_duplicates()\n",
    "unique_cities = unique_city_businesses.groupby('city').count()['business_id']\n",
    "unique_cities = unique_cities[unique_cities > 100]\n",
    "out = pd.DataFrame()\n",
    "for city in unique_cities.index:\n",
    "    tmp = ratings_holdout[(ratings_holdout['city'] ==city) &\n",
    "                              (ratings_holdout['rating'] >ratings_holdout['average_stars'])]\n",
    "    if len(tmp['user_id'].unique())>4:\n",
    "        \n",
    "        ###this weird sampling technique is to ensure we dont' sample the same user twice in a same city\n",
    "        five_users = np.random.choice(tmp['user_id'].unique(),5, replace = False)\n",
    "        row = tmp[tmp['user_id'].isin(five_users)].groupby('user_id', group_keys=False).apply(lambda df: df.sample(1))\n",
    "        out = out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = out[['user_id','city','state']]\n",
    "predict_df = predict_df.merge(unique_city_businesses, on = 'city')\n",
    "predict_df.to_csv('data/metric_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df['predictions'] = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(predict_df, validation_subsample, ratings_train_final):\n",
    "    top_10_recs = predict_df.groupby(['user_id','city'])['predictions'].nlargest(10).reset_index()\n",
    "    out = validation_subsample\n",
    "    cnt =0\n",
    "    serendipity = 0\n",
    "    \n",
    "    \n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        top_10 = predict_df.loc[top_10_recs[top_10_recs['user_id'] == row_values['user_id']].level_2]['business_id']\n",
    "        ###In top 10\n",
    "        if row_values['business_id'] in top_10.values:\n",
    "            cnt+=1\n",
    "        user_history = ratings_train_final[ratings_train_final['user_id'] == row_values['user_id']]    \n",
    "        been_there = [i for i in top_10.values if i in  user_history.business_id.values]\n",
    "        serendipity += 1-len(been_there)/10\n",
    "    \n",
    "    top_10 = cnt/len(out)\n",
    "    serendipity = serendipity/len(out)\n",
    "    \n",
    "    predict_df = predict_df.reset_index()\n",
    "    \n",
    "    analysis_df = predict_df.merge(top_10_recs, left_on = ['user_id','city','index'], \\\n",
    "                                   right_on = ['user_id','city','level_2'])\n",
    "    \n",
    "    coverage = (analysis_df.groupby('city')['business_id'].nunique()/50).values.mean()\n",
    "    \n",
    "    predict_df['rankings']=predict_df.groupby(['city','user_id'])['predictions']. \\\n",
    "                                                        rank(method=\"first\",ascending = False)\n",
    "    running_rankings =0\n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        user_recs = predict_df[(predict_df['user_id']==row_values['user_id'])\n",
    "                            &(predict_df['city']==row_values['city'])\n",
    "                             & (predict_df['business_id']==row_values['business_id'])\n",
    "                              ]\n",
    "        assert len(user_recs)==1\n",
    "        running_rankings += user_recs['rankings'].sum()\n",
    "\n",
    "    avg_rank = running_rankings / len(out)\n",
    "    print(top_10, coverage, serendipity, avg_rank)\n",
    "    \n",
    "    return top_10, coverage, serendipity, avg_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Methods\n",
    "\n",
    "A list of models that the team attempts are: \n",
    "\n",
    "* Bias Baseline\n",
    "* Collaborative Filtering Baseline: SVD\n",
    "* Collective Matrix Factorization (CMF)\n",
    "* Content-Based Filtering (CBF)\n",
    "* Field-aware Factorization Machine (FFM) \n",
    "* Deep Learning Model\n",
    "\n",
    "_**Note**_: the team has run algorithms on **20%, 50%, 100%** training data respectively. But for readability purpose, only result on 20% data is displayed in this notebook. For full result, please refer to a result table enclosed in **pdf report**.\n",
    "\n",
    "### Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: Bias Baseline\n",
    "\n",
    "$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Dataset\n",
    "from surprise import BaselineOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':3}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':5}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':7}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':9}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: It seems different hyperparameters all performs the same result; the team just uses default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "**Note**: the team has evaluated this and the following algorithms on **20%, 50%, 100%** training data respectively. For readability, only result on 20% data is displayed. For full result, please refer to a result table enclosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "--- 2.0564420223236084 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 20%\n",
    "start_time = time.time()\n",
    "bias_baseline.fit(train_sr_20)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3545\n",
      "R^2 (with 20% data):  0.19799189125456051\n",
      "MAE (with 20% data):  1.127068744947832\n",
      "--- 0.08617496490478516 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 20%\n",
    "bbase_p = bias_baseline.test(test_sr_20)\n",
    "start_time = time.time()\n",
    "bbase_20_df = pd.DataFrame(bbase_p, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "accuracy.rmse(bbase_p)\n",
    "print('R^2 (with 20% data): ', r2_score(bbase_20_df.rating , bbase_20_df.pred_rating))\n",
    "print('MAE (with 20% data): ', mean_absolute_error(bbase_20_df.rating, bbase_20_df.pred_rating))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "algo = BaselineOnly()\n",
    "eval_before_50 = eval_20.build_full_trainset()\n",
    "eval_sr_20 = eval_before_20.build_testset()\n",
    "algo.fit(train_sr_20)\n",
    "eval_pred_20 = algo.test(eval_sr_20)\n",
    "#accuracy.rmse(predictions_50)\n",
    "baseline_20 = pd.DataFrame(eval_pred_20, columns = ['userId','itemId','rating','pred_rating','x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10714285714285714 0.503095238095238 0.9697619047619042 528.3333333333334\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out_20, ratings_train_final_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: Collaborative Filtering via SVD\n",
    "\n",
    "Matrix factorization is a class of collaborative filtering algorithms. The general idea behind matrix factorization is that there can exist a lower dimensional latent space of features in which users and items can be represented such that the interaction between them can be obtained by simply dot producing the corresponding dense vectors in that space. In short, it decomposes a m*n user-item interaction matrix into two m*k and k*n matrices, sharing a joint latent vector space, where m represents the number of users, and n represents the number of items. In terms of its outcome, we are likely to observe that close users in terms of preferences as well as close items in terms of characteristics can have close representations in the latent space.\n",
    "\n",
    "The mathematical overview is as follows:\n",
    "Given a n*m matrix, such that . X is the user matrix where rows represent the n users and Y is the item matrix where rows represent the m items. We want to search for the dot product of matrices X and Y that best approximate the existing interactions; i.e., we want to find X and Y that minimize the “rating reconstruction error”:\n",
    "\n",
    "$$ (X,Y) = argmin_{X,Y} \\sum_{(i,j) \\in E} [(X_i)(Y_j)^T − M_{ij}]^2$$\n",
    "\n",
    "Adding a regularization term, we can also get:\n",
    "\n",
    "$$(X,Y) = argmin_{X,Y} ½ \\sum_{(i,j) \\in E} [(X_i)(Y_j)^T − M_{ij}]^2 + \\lambda/2(\\sum_{i,k}(X_{ik})^2 + \\sum_{j,k}(Y_{jk})^2)$$\n",
    "\n",
    "In general, we obtain the matrices X and Y following a gradient descent optimization process. And once the matrices are obtained, we can predict the ratings simply by multiplying the user vector by any item vector.\n",
    "\n",
    "In this Yelp Rating Challenge, we used the python surprise package to implement MF. The MF algorithm there uses the SVD approach, which is essentially \n",
    "\n",
    "$$ P_{m * n} = U_{m * m} \\sum_{m * n} V_{n * n}$$\n",
    "\n",
    "There, the prediction is\n",
    " $$\\hat(r_{ui}) = \\mu + b_u + b_i + (q_i)^T p_u $$\n",
    " \n",
    "and the regularized squared error that needs to be minimized is \n",
    "\n",
    "$$\\sum_{r_{ui} \\in R_{train}} (r_{ui} − \\hat(r_{ui}))^2 + \\lambda(b^2_{i} + b^2_{u} + ||q_i||^2 + ||p_u||^2)$$\n",
    "\n",
    "As the way the package is designed, we tuned on n_epochs, lr_all and leg_all to get an optimal hyperparameter set, where n_epochs is the number of iterations of the SGD (stochastic gradient descent) procedure, lr_all is the learning rate for all parameters, and reg_all is the regularization term for all parameters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cmfrec\n",
    "# !pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzljohn18/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from cmfrec import CMF\n",
    "from surprise import KNNBasic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "              'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_test = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test = ratings_test.loc[ratings_test.business_id.isin(ratings_train.business_id)]\n",
    "ratings_val = ratings_val.loc[ratings_val.business_id.isin(ratings_train.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ratings_train.loc[:,['user_id', 'business_id', 'rating']]\n",
    "trainset.columns = ['userID', 'itemID','rating']\n",
    "valset = ratings_val.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "valset.columns = ['userID', 'itemID','rating']\n",
    "testset = ratings_holdout.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "testset.columns = ['userID', 'itemID','rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale = (0.0, 5.0))\n",
    "train_data = Dataset.load_from_df(trainset[['userID','itemID','rating']], reader)\n",
    "val_data = Dataset.load_from_df(valset[['userID','itemID','rating']], reader)\n",
    "test_data = Dataset.load_from_df(testset[['userID','itemID','rating']], reader)\n",
    "\n",
    "train_sr = train_data.build_full_trainset()\n",
    "val_sr_before = val_data.build_full_trainset()\n",
    "val_sr = val_sr_before.build_testset()\n",
    "test_sr_before = test_data.build_full_trainset()\n",
    "test_sr = test_sr_before.build_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4171\n",
      "RMSE: 1.4166\n",
      "RMSE: 1.4172\n",
      "RMSE: 1.4012\n",
      "RMSE: 1.4023\n",
      "RMSE: 1.4032\n",
      "RMSE: 1.3794\n",
      "RMSE: 1.3805\n",
      "RMSE: 1.3820\n",
      "RMSE: 1.4048\n",
      "RMSE: 1.4047\n",
      "RMSE: 1.4054\n",
      "RMSE: 1.3888\n",
      "RMSE: 1.3888\n",
      "RMSE: 1.3902\n",
      "RMSE: 1.3643\n",
      "RMSE: 1.3664\n",
      "RMSE: 1.3681\n",
      "RMSE: 1.3897\n",
      "RMSE: 1.3906\n",
      "RMSE: 1.3914\n",
      "RMSE: 1.3721\n",
      "RMSE: 1.3733\n",
      "RMSE: 1.3749\n",
      "RMSE: 1.3496\n",
      "RMSE: 1.3513\n",
      "RMSE: 1.3529\n"
     ]
    }
   ],
   "source": [
    "RMSE_tune = {}\n",
    "n_epochs = [5, 7, 10]  # the number of iteration of the SGD procedure\n",
    "lr_all = [0.002, 0.003, 0.005] # the learning rate for all parameters\n",
    "reg_all =  [0.4, 0.5, 0.6] # the regularization term for all parameters\n",
    "\n",
    "for n in n_epochs:\n",
    "    for l in lr_all:\n",
    "        for r in reg_all:\n",
    "            algo = SVD(n_epochs = n, lr_all = l, reg_all = r)\n",
    "            algo.fit(train_sr)\n",
    "            predictions = algo.test(val_sr)\n",
    "            RMSE_tune[n,l,r] = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 0.005, 0.4)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "min(RMSE_tune.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The best combination is when n_epochs = 10, lr_all = 0.005, reg_all = 0.4, and the RMSE score is 1.3496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test on the optimal parameter\n",
    "start_time = time.time()\n",
    "algo_real = SVD(n_epochs = 10, lr_all = 0.005, reg_all = 0.4)\n",
    "algo_real.fit(train_sr)\n",
    "predictions = algo_real.test(test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 33.18766689300537 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.398543135413844"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  1.1848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1847820428584857"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'city', 'state', 'business_id'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df_20 = pd.read_csv('data/metric_sample.csv', index_col=0)\n",
    "predict_df_20.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate coverage and serendipity metrics, use evaluation set created earlier.\n",
    "predict_df_20 = pd.read_csv('data/metric_sample.csv', index_col=0)\n",
    "predict_df_20['predictions'] = 2.5 # fill in this value temporally\n",
    "eval_20 = Dataset.load_from_df(predict_df_20[['user_id','business_id','predictions']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_before_20 = eval_20.build_full_trainset()\n",
    "eval_sr_20 = eval_before_20.build_testset()\n",
    "eval_pred_20 = algo_real.test(eval_sr_20)\n",
    "\n",
    "baseline_20 = pd.DataFrame(eval_pred_20, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "predict_df_20['predictions'] = baseline_20.pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10930232558139535 0.4932558139534884 0.9762790697674414 518.7511627906977\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out, ratings_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "### (i) Collective Matrix Factorization (CMF)\n",
    "\n",
    "Collective Matrix Factorization method decomposes two matrices $X$ and $Y$ into three matrices $U$, $V$, and $Z$ such that $X \\approx f(UV^T)$ and $ Y \\approx f(VZ^T)$, where f is either the identity or sigmoid function. This allows us to include additional features of our users or items that might help optimize the result of personalization. \n",
    "\n",
    "**Approach 1: CMF on User, Business and State Average Rating**<br>\n",
    "In the first approach, we calculated state average rating as an additional feature on the items (i.e., the restaurants). The idea is as follows: our goal is to predict the last rating for each active user. To bring closer our prediction to the actual rating, we felt that state average rating can help. In other words, \"eaters\" should have similar pickiness when it comes to foods and restaurants in the same state. They know the best about the expectation they should have for restaurants around them as the locals, and we wanted to take into account of this aspect. In particular, we didn't do the average calculation by summing up from the business dataset, because that will be somewhat \"leaking\" the information, since we are holding out the last rating of our active users. Thus, we calculated the ratings from the training ratings by user by state.\n",
    "\n",
    "For example, (and in fact from our EDA, though with limited data), people in CA seem to be less critical with foods and restaurants as they tend to have a higher state average rating (perhaps, it already takes away a lot of energy to drive around to the restaurants as CA is such a massive land, so they just complain less), whereas people in NY really have opinions and take very personally with what they intake as their state average rating tends to be lower (perhaps, and it seems reasonable, that NY is known as the food hub, so people definitely will have a higher expectation for foods). In other words, we are trying to make up this very \"subjective\" assumption about the eating behaviors of the yelp users (that they really know what they are eating) and we will test the performance of our model to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users_\n",
    "businesses = business_\n",
    "\n",
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users, on = 'user_id')\n",
    "    df = df.merge(businesses, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "                  'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_holdout_100 = ratings_holdout_\n",
    "ratings_train_100 = ratings_train_\n",
    "ratings_val_100 = ratings_cv_\n",
    "\n",
    "ratings_val_100 = process(ratings_val_100.copy())\n",
    "ratings_train_100 = process(ratings_train_100.copy())\n",
    "ratings_holdout_100 = process(ratings_holdout_100.copy())\n",
    "\n",
    "ratings_test_100 = ratings_holdout_100.loc[ratings_holdout_100.business_id.isin(ratings_train_100.business_id)]\n",
    "ratings_val_100 = ratings_val_100.loc[ratings_val_100.business_id.isin(ratings_train_100.business_id)]\n",
    "\n",
    "trainset_100 = ratings_train_100.loc[:,['user_id', 'business_id', 'rating']]\n",
    "trainset_100.columns = ['userID', 'itemID','rating']\n",
    "valset_100 = ratings_val_100.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "valset_100.columns = ['userID', 'itemID','rating']\n",
    "testset_100 = ratings_holdout_100.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "testset_100.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "ratings_train_final_100 = ratings_train_100.append(ratings_val_100)\n",
    "ratings_entire_df_100 = ratings_train_100.append(ratings_val_100).append(ratings_holdout_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Content Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1: Predict rating via user reviews**<br>\n",
    "\n",
    "Implemented according to **CF-MCM (User-Item based)** approach in this following paper: https://arxiv.org/pdf/1607.00024.pdf\n",
    "\n",
    "The idea of this model is to predict the rating of a review given by user _U_ to restaurant _R_ by comparing the review to the ones written by other users who have also reviewed restaurant _R_. The predicted rating is calculated by taking a weighted average of other users' ratings, where the weight is calculated based on similarity of reviews. Here is the detail: \n",
    "\n",
    "<img src=\"image/formula.png\" width=\"700\">\n",
    "\n",
    "Two users'ratings are compared in the following manner: first aggregate all reviews user _u_ and user _a_ have written; then bucket them according to ratings given (from r = 1.0 to 5.0); finally find the max of all 5 pairs of similarity. \n",
    "\n",
    "<img src=\"image/text_comparison.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jzljohn18/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = ratings_train_.copy()\n",
    "ratings_cv = ratings_cv_.copy()\n",
    "ratings_holdout = ratings_holdout_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # substitute some irregular context \n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # remove punctuations from each word\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "    \n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    # Replace slang terms\n",
    "    # Word stemming\n",
    "    \n",
    "#     porter = PorterStemmer()\n",
    "#     text = [porter.stem(word) for word in text]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "ratings_train['text'] = ratings_train['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>say office really together organized friendly ...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30Q5xBagQHmkwp8Q9I1FCg</td>\n",
       "      <td>2018-02-03 23:27:43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>first time restaurant server ramone asked firs...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UtWngqS-WloIY_A53W5K-Q</td>\n",
       "      <td>2016-02-18 06:42:16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>amazing hospital friendly staff nurses doctors...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76aGN20BrGWvAlYfPGVv_A</td>\n",
       "      <td>2016-04-05 15:25:11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>let start saying grew father mechanic retired ...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FUcwrXBb_ljg3LgTqt6F2g</td>\n",
       "      <td>2018-08-15 22:07:13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>love pharmacy compounded prescription cats hyp...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 date  rating  \\\n",
       "0  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
       "2  30Q5xBagQHmkwp8Q9I1FCg  2018-02-03 23:27:43     5.0   \n",
       "3  UtWngqS-WloIY_A53W5K-Q  2016-02-18 06:42:16     5.0   \n",
       "5  76aGN20BrGWvAlYfPGVv_A  2016-04-05 15:25:11     5.0   \n",
       "6  FUcwrXBb_ljg3LgTqt6F2g  2018-08-15 22:07:13     5.0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  say office really together organized friendly ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "2  first time restaurant server ramone asked firs...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "3  amazing hospital friendly staff nurses doctors...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "5  let start saying grew father mechanic retired ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "6  love pharmacy compounded prescription cats hyp...  n6-Gk65cPZL6Uz8qRm3NYw  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatnate all reviews for each business for each rating value\n",
    "ratings_train_sub = ratings_train[['user_id', 'business_id', 'rating', 'text']]\n",
    "# ratings_train_sub['ct'] = 0\n",
    "userid_df = ratings_train_sub.groupby(['user_id', 'rating']).agg({'text': ' '.join, 'business_id': 'count'})\n",
    "# retain a copy of indexed dataset\n",
    "businessid_userid_df = ratings_train_sub.set_index(['user_id', 'business_id', 'rating'])\n",
    "\n",
    "## Learn TF-IDF vector representation for each concatenated review\n",
    "# which will be used for similarity calculation\n",
    "userid_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, analyzer='word', \\\n",
    "                             stop_words='english', max_features=50)\n",
    "userid_tfidf_fit = userid_vectorizer.fit(userid_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm implementation \n",
    "\n",
    "def predict_approach_1(selected_uid, selected_bid): \n",
    "    # cosine similarity\n",
    "    def text_similarity(v1, v2):\n",
    "        return np.dot(v1, v2.T).toarray()[0][0]\n",
    "    \n",
    "    pred_rating = -1\n",
    "\n",
    "    users_reviews = userid_df\n",
    "\n",
    "    selected_business = businessid_userid_df.xs(selected_bid, level='business_id').reset_index()\n",
    "    users_who_review_business = selected_business.user_id\n",
    "\n",
    "    user_reviews_userID = users_reviews.xs(selected_uid, level='user_id')\n",
    "\n",
    "    w_ui_u_vec = []\n",
    "    for ui in users_who_review_business:\n",
    "        user_reviews_ui = users_reviews.xs(ui, level='user_id')\n",
    "        rating_range = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "        valid_rating_range_1 = user_reviews_userID.reset_index()['rating'].tolist()\n",
    "        valid_rating_range_2 = user_reviews_ui.reset_index()['rating'].tolist()\n",
    "        w_ui_u = -100\n",
    "\n",
    "        for r in rating_range:\n",
    "            if (r not in valid_rating_range_1) or (r not in valid_rating_range_2):\n",
    "                continue\n",
    "            else:\n",
    "                user_reviews_r = user_reviews_userID.loc[r, 'text']\n",
    "                ui_reviews_r = user_reviews_ui.loc[r, 'text']\n",
    "                user_reviews_r_vec = userid_tfidf_fit.transform([user_reviews_r])\n",
    "                ui_reviews_r_vec = userid_tfidf_fit.transform([ui_reviews_r])\n",
    "                # similarity_r = text_similarity(user_reviews_r, ui_reviews_r)\n",
    "                similarity_r = text_similarity(user_reviews_r_vec, ui_reviews_r_vec)\n",
    "                if similarity_r > w_ui_u:\n",
    "                    w_ui_u = similarity_r\n",
    "        w_ui_u_vec.append(w_ui_u)\n",
    "\n",
    "    tmp = user_reviews_userID.reset_index()\n",
    "    r_u_bar = sum(tmp.rating * tmp.business_id) / sum(tmp.business_id) # business_id is number of ratings \n",
    "                                                                        # that has correpsonding value \n",
    "    if sum(w_ui_u_vec) == 0:\n",
    "        pred_rating = r_u_bar + sum(w_ui_u_vec \\\n",
    "                                * (selected_business.rating - np.mean(selected_business.rating))) \\\n",
    "                                * 1.0 / sum(w_ui_u_vec)\n",
    "    else:\n",
    "        pred_rating = r_u_bar\n",
    "    return pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ratings_cv[ratings_cv['user_id'].isin(ratings_train.user_id)].reset_index()\n",
    "ratings_cv_filtered = tmp_df[tmp_df['business_id'].isin(ratings_train.business_id)]\n",
    "\n",
    "tmp_df = ratings_holdout[ratings_holdout['user_id'].isin(ratings_train.user_id)].reset_index()\n",
    "ratings_holdout_filtered = tmp_df[tmp_df['business_id'].isin(ratings_train.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1172it [07:08,  3.02it/s]/home/jzljohn18/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "53225it [4:48:43,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approach 1 takes 288.7285438974698 minutes to run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "approach_1_start = time.time()\n",
    "\n",
    "predict_df_1 = ratings_cv_filtered.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "predict_df_1['pred_rating'] = -1\n",
    "\n",
    "for (u, b) in tqdm(predict_df_1.iterrows(), position=0, leave=True):\n",
    "    try:\n",
    "        pred = predict_approach_1(b.user_id, b.business_id)\n",
    "        predict_df_1.loc[u, 'pred_rating'] = pred \n",
    "    except AttributeError:\n",
    "        if sum(predict_df_1.pred_rating == -1) == 0:\n",
    "            print('Finished')\n",
    "        else:\n",
    "            print('Error: Not Finised')\n",
    "            print('Currently at u = {}'.format(u))\n",
    "approach_1_duration = (time.time() - approach_1_start) * 1.0 / 60 \n",
    "print('The approach 1 takes {} minutes to run.'.format(approach_1_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>pred_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>dU-Nt1-LjV9mAgFOVcdAJw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "      <td>d10IxZPirVJlOSpdRZJczA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "      <td>BxJAl-LoiSiIHonoGzVu7Q</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIk4lQQu1eTe2EpzQ4xhBA</td>\n",
       "      <td>gcouHCQrswvakJ6xSEKtFQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.124476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TpyOT5E16YASd7EWjLQlrw</td>\n",
       "      <td>sYSlKRCWmVeQr1hA6-WUzw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.725275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  rating  pred_rating\n",
       "0  n6-Gk65cPZL6Uz8qRm3NYw  dU-Nt1-LjV9mAgFOVcdAJw     5.0     3.857143\n",
       "1  d6xvYpyzcfbF_AZ8vMB7QA  d10IxZPirVJlOSpdRZJczA     1.0     2.800000\n",
       "2  sG_h0dIzTKWa3Q6fmb4u-g  BxJAl-LoiSiIHonoGzVu7Q     2.0     3.400000\n",
       "3  FIk4lQQu1eTe2EpzQ4xhBA  gcouHCQrswvakJ6xSEKtFQ     4.0     4.124476\n",
       "4  TpyOT5E16YASd7EWjLQlrw  sYSlKRCWmVeQr1hA6-WUzw     4.0     3.725275"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53225, 4)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53162, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows without predictions\n",
    "predict_df_1_NA_removed = predict_df_1.loc[~predict_df_1.pred_rating.isnull(), :]\n",
    "predict_df_1_NA_removed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.1025, MAE: 1.1308, R2: 0.0218\n"
     ]
    }
   ],
   "source": [
    "mse, mae, r2 = mean_squared_error(predict_df_1_NA_removed.rating, predict_df_1_NA_removed.pred_rating), \\\n",
    "                mean_absolute_error(predict_df_1_NA_removed.rating, predict_df_1_NA_removed.pred_rating), \\\n",
    "                r2_score(predict_df_1_NA_removed.rating, predict_df_1_NA_removed.pred_rating)\n",
    "print('MSE: {0:.4f}, MAE: {1:.4f}, R2: {2:.4f}'.format(mse, mae, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df_target = pd.read_csv('data/metric_sample.csv', index_col=0)\n",
    "# # predict_df_target['predictions'] = 2.5 # fill in this value temporally\n",
    "# # predict_df_1.loc[predict_df_1.user_id == predict_df_target.user_id && \\\n",
    "# #                  predict_df_1.business_id == predict_df_target.business_id, :]\n",
    "\n",
    "# predict_df_target_combined = pd.merge(predict_df_target, predict_df_1,  how='left', left_on=['user_id','business_id'], \\\n",
    "#          right_on = ['user_id','business_id'])\n",
    "# # top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_target, out, ratings_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2: (Implementation please see Appendix)**\n",
    "\n",
    "Build User Profile & Business Profile based on business categories information. As the algorithm takes too long to run, the code is attached in appendix. The idea is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Field-Aware Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install CMake\n",
    "# Installation guide https://linxid.github.io/2018/05/10/Xlearn/\n",
    "# !pip install xlearn\n",
    "# !pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30Q5xBagQHmkwp8Q9I1FCg</td>\n",
       "      <td>2018-02-03 23:27:43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>First time at this restaurant our server \"Ramo...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UtWngqS-WloIY_A53W5K-Q</td>\n",
       "      <td>2016-02-18 06:42:16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Such an amazing hospital with friendly staff, ...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76aGN20BrGWvAlYfPGVv_A</td>\n",
       "      <td>2016-04-05 15:25:11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Let me start by saying that I grew up with a f...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FUcwrXBb_ljg3LgTqt6F2g</td>\n",
       "      <td>2018-08-15 22:07:13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this pharmacy, they compounded a prescrip...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 date  rating  \\\n",
       "0  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
       "2  30Q5xBagQHmkwp8Q9I1FCg  2018-02-03 23:27:43     5.0   \n",
       "3  UtWngqS-WloIY_A53W5K-Q  2016-02-18 06:42:16     5.0   \n",
       "5  76aGN20BrGWvAlYfPGVv_A  2016-04-05 15:25:11     5.0   \n",
       "6  FUcwrXBb_ljg3LgTqt6F2g  2018-08-15 22:07:13     5.0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "2  First time at this restaurant our server \"Ramo...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "3  Such an amazing hospital with friendly staff, ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "5  Let me start by saying that I grew up with a f...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "6  Love this pharmacy, they compounded a prescrip...  n6-Gk65cPZL6Uz8qRm3NYw  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "import networkx as nx\n",
    "pd.options.mode.chained_assignment = None\n",
    "import regex as re\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'date', 'rating', 'text', 'user_id', 'week_day', 'month',\n",
       "       'hour', 'user_name', 'user_review_count', 'user_yelping_since',\n",
       "       'friends', 'useful_reviews', 'funny_reviews', 'cool_reviews', 'n_fans',\n",
       "       'years_elite', 'average_stars', 'business_name', 'business_address',\n",
       "       'business_city', 'business_state', 'business_latitude',\n",
       "       'business_longitude', 'stars', 'review_counts', 'is_open', 'categories',\n",
       "       'state_avg', 'text_cluster', 'graph_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "                  'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_holdout = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_ffm(path, df, type, target, numerics, categories, features, encoder):\n",
    "    # Flagging categorical and numerical fields\n",
    "    print('convert_to_ffm - START')\n",
    "    for x in numerics:\n",
    "        if(x not in encoder['catdict']):\n",
    "            print(f'UPDATING CATDICT: numeric field - {x}')\n",
    "            encoder['catdict'][x] = 0\n",
    "    for x in categories:\n",
    "        if(x not in encoder['catdict']):\n",
    "            print(f'UPDATING CATDICT: categorical field - {x}')\n",
    "            encoder['catdict'][x] = 1\n",
    "\n",
    "    nrows = df.shape[0]\n",
    "    with open(path + str(type) + \"_ffm.txt\", \"w\") as text_file:\n",
    "\n",
    "        # Looping over rows to convert each row to libffm format\n",
    "        for n, r in enumerate(range(nrows)):\n",
    "            datastring = \"\"\n",
    "            datarow = df.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow[target]))  # Set Target Variable here\n",
    "\n",
    "            # For numerical fields, we are creating a dummy field here\n",
    "            for i, x in enumerate(encoder['catdict'].keys()):\n",
    "                if(encoder['catdict'][x] == 0):\n",
    "                    # Not adding numerical values that are nan\n",
    "                    if math.isnan(datarow[x]) is not True:\n",
    "                        datastring = datastring + \" \"+str(i)+\":\" + str(i)+\":\" + str(datarow[x])\n",
    "                else:\n",
    "\n",
    "                    # For a new field appearing in a training example\n",
    "                    if(x not in encoder['catcodes']):\n",
    "                        print(f'UPDATING CATCODES: categorical field - {x}')\n",
    "                        encoder['catcodes'][x] = {}\n",
    "                        encoder['currentcode'] += 1\n",
    "                        print(f'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}')\n",
    "                        encoder['catcodes'][x][datarow[x]] = encoder['currentcode']  # encoding the feature\n",
    "\n",
    "                    # For already encoded fields\n",
    "                    elif(datarow[x] not in encoder['catcodes'][x]):\n",
    "                        encoder['currentcode'] += 1\n",
    "                        print(f'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}')\n",
    "                        encoder['catcodes'][x][datarow[x]] = encoder['currentcode']  # encoding the feature\n",
    "\n",
    "                    code = encoder['catcodes'][x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\" + str(int(code))+\":1\"\n",
    "\n",
    "            datastring += '\\n'\n",
    "            text_file.write(datastring)\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Generation \n",
    "\n",
    "In the following steps, different sets of features will be generated as input to FFM.\n",
    "\n",
    "\n",
    "(i) Create user review **text embeddings** based on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_texts = ratings_train.groupby(['user_id'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "rating_texts['text'] = rating_texts['text'].str.lower()\n",
    "rating_texts['text'] = rating_texts['text'].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words = 'english', strip_accents = 'ascii')\n",
    "vectorizer.fit(rating_texts['text'])\n",
    "vector = vectorizer.transform(rating_texts['text'])\n",
    "tsv = TruncatedSVD(n_components=50)\n",
    "tsv.fit(vector)\n",
    "transformed_tsv = tsv.transform(vector)\n",
    "\n",
    "'''\n",
    "wcss=[]\n",
    "for i in range(77, 100):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(transformed_tsv)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 100), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "kmeans = KMeans(n_clusters=110, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(transformed_tsv)\n",
    "text_cluster = kmeans.predict(transformed_tsv)\n",
    "\n",
    "rating_texts.loc[:,'text_cluster'] = text_cluster\n",
    "rating_texts_features = rating_texts[['user_id','text_cluster']]\n",
    "ratings_train= ratings_train.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')\n",
    "ratings_holdout = ratings_holdout.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')\n",
    "ratings_val = ratings_val.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>week_day</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_counts</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "      <th>state_avg</th>\n",
       "      <th>text_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>Wilhelmina</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.259702</td>\n",
       "      <td>-111.790203</td>\n",
       "      <td>3.5</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Medical, Cosmetic Dentists, Orthodont...</td>\n",
       "      <td>3.707185</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                date  rating  \\\n",
       "0  WTqjgwHlXbSFevF32_DJVw 2016-11-09 20:09:03     5.0   \n",
       "\n",
       "                                                text                 user_id  \\\n",
       "0  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw   \n",
       "\n",
       "   week_day  month  hour   user_name  user_review_count     ...       \\\n",
       "0         2     11    20  Wilhelmina                 10     ...        \n",
       "\n",
       "       city state   latitude   longitude  stars  review_counts is_open  \\\n",
       "0  Chandler    AZ  33.259702 -111.790203    3.5             39       1   \n",
       "\n",
       "                                          categories state_avg text_cluster  \n",
       "0  Health & Medical, Cosmetic Dentists, Orthodont...  3.707185           82  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Create **graph** features based on user friends attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = ratings_train[['user_id','friends']].drop_duplicates()\n",
    "train_users_dict = train_users.set_index('user_id').T.to_dict('list')\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(train_users_dict.keys())\n",
    "\n",
    "for k, v in train_users_dict.items():\n",
    "    for i in v[0].split(','):\n",
    "        g.add_edge(k,i.strip())    \n",
    "\n",
    "sub_graphs = nx.connected_component_subgraphs(g)\n",
    "\n",
    "sgs =[]\n",
    "for i, sg in enumerate(sub_graphs):\n",
    "    sgs += [sg]\n",
    "fin_sgs = []\n",
    "for i in sgs:\n",
    "    if len(i.nodes()) >=5:\n",
    "        fin_sgs +=[i]\n",
    "\n",
    "graph_user_ids, graph_ids = [],[]\n",
    "num_id = 0\n",
    "for graph in fin_sgs:\n",
    "    for node in graph.nodes():\n",
    "        graph_user_ids += [node]\n",
    "        graph_ids += [num_id]\n",
    "    num_id += 1\n",
    "social_graphs = pd.DataFrame(\n",
    "    {\"user_id\": graph_user_ids, \"graph_cluster\": graph_ids}\n",
    ")\n",
    "\n",
    "ratings_train= ratings_train.merge(social_graphs, on ='user_id', how = 'left')\n",
    "ratings_holdout = ratings_holdout.merge(social_graphs, on ='user_id', how = 'left')\n",
    "ratings_val = ratings_val.merge(social_graphs, on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(iii) Create **Location** Features: business longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ratings_train[['longitude','latitude']]\n",
    "\n",
    "''' \n",
    "EXPLORATORY ANALYSIS TO DETERMINE NUMBER OF CLUSTERS. DON'T RUN\n",
    "HERE USING THE EBLOW METHOD WE CHOOSE NCLUSTERs OF 10 \n",
    "\n",
    "X = ratings_train[['longitude','latitude']]\n",
    "wcss = []\n",
    "for i in range(2, 100):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 50), wcss[0:48])\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "ratings_train.loc[:,'location_cluster'] = kmeans.predict(ratings_train[['longitude','latitude']])\n",
    "ratings_holdout.loc[:,'location_cluster'] = kmeans.predict(ratings_holdout[['longitude','latitude']])\n",
    "ratings_val.loc[:,'location_cluster'] = kmeans.predict(ratings_val[['longitude','latitude']])\n",
    "\n",
    "ratings_train.text_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.text_cluster.fillna('999', inplace=True)\n",
    "ratings_val.text_cluster.fillna('999', inplace=True)\n",
    "\n",
    "ratings_train.location_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.location_cluster.fillna('999', inplace=True)\n",
    "ratings_val.location_cluster.fillna('999', inplace=True)\n",
    "\n",
    "ratings_train.graph_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.graph_cluster.fillna('999', inplace=True)\n",
    "ratings_val.graph_cluster.fillna('999', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMModel:\n",
    "\n",
    "    def __init__(self, train, test, config, suffix = None):\n",
    "        self.train_df = train\n",
    "        self.test_df = test\n",
    "        self.model = xl.create_ffm()\n",
    "        self.suffix = suffix\n",
    "        self.config = config\n",
    "        self.preds = None \n",
    "          \n",
    "    def __configure(self):\n",
    "        destination = self.config['destination']\n",
    "        label = self.config['label']\n",
    "        numerical_columns  = self.config['numerical_columns']\n",
    "        categorical_columns  = self.config['categorical_columns']\n",
    "        all_columns  = numerical_columns + categorical_columns\n",
    "              \n",
    "        encoder = {\"currentcode\": len(self.config['numerical_columns']),\n",
    "           \"catdict\": {},\n",
    "           \"catcodes\": {}}\n",
    "        \n",
    "        encoder = _convert_to_ffm(destination, self.train_df , 'train', label, numerical_columns, \\\n",
    "                                  categorical_columns, all_columns, encoder)\n",
    "        encoder = _convert_to_ffm(destination, self.test_df , 'test', label, numerical_columns, \\\n",
    "                                  categorical_columns, all_columns, encoder)\n",
    "        \n",
    "    def train(self, params = None):\n",
    "        encoder = self.__configure()\n",
    "        self.model.setTrain(self.config['destination']+'train_ffm.txt')\n",
    "        self.model.setValidate(self.config['destination']+'test_ffm.txt')\n",
    "        self.model.setTest(self.config['destination']+'test_ffm.txt')\n",
    "        \n",
    "        if not params:\n",
    "            params = {'task': 'reg',\n",
    "                     'lr': 0.2,\n",
    "                     'lambda': 0.002,\n",
    "                     'metric': 'auc'}\n",
    "        \n",
    "        self.model.fit(params, self.config['model_destination']+self.config['model_name']+'.out')\n",
    "\n",
    "    def evaluate_on_val(self):\n",
    "        self.model.predict(self.config['model_destination'] + self.config['model_name']+'.out', \\\n",
    "                           self.config['output_destination']+'predictions.txt')\n",
    "        preds = pd.read_csv(self.config['output_destination']+'predictions.txt', sep=\" \", header=None)\n",
    "        preds.columns = ['prediction']\n",
    "        preds.prediction = np.clip(preds.prediction,0.0,5.0)\n",
    "        self.preds = preds\n",
    "        return preds            \n",
    "    \n",
    "    def get_regression_metrics(self):\n",
    "        if self.preds is None:\n",
    "            self.evaluate_on_val()\n",
    "        \n",
    "        predictions = self.evaluate_on_val()\n",
    "        test_df = self.test_df.copy()\n",
    "        test_df['preds']  = np.clip(predictions.values,0.0,5.0)\n",
    "        \n",
    "        r2 = r2_score(test_df['rating'], test_df['preds'])\n",
    "        mse = mean_squared_error(test_df['rating'], test_df['preds'])\n",
    "        mae = mean_absolute_error(test_df['rating'], test_df['preds'])\n",
    "        \n",
    "        print('R2: ', r2, ' MSE: ',mse, ' MAE: ', mae)\n",
    "        return r2,mse,mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting and Evaluation\n",
    "\n",
    "Now run FFM on different combinations of features and calculate R2, mean squared error (MSE) and mean absolute error (MAE) on them.\n",
    "\n",
    "#### Metrics with just userid and business id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'baseline',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_FFM = FFMModel(ratings_train, ratings_val, baseline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "baseline_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.1873866119640475  MSE:  1.7830047851238464  MAE:  1.0886505079082125\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = baseline_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with Baseline + Business Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_business_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'user_business',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_business_FFM = FFMModel(ratings_train, ratings_val, user_business_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "user_business_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.1964585848489021  MSE:  1.763099414005971  MAE:  1.0615091977315227\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = user_business_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with Baseline + Location Cluster (has best MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','location_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'location',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_FFM = FFMModel(ratings_train, ratings_val, location_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "location_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.19744277000305088  MSE:  1.760939953104722  MAE:  1.0666155431063107\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = location_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with User and Business Information + Text Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','text_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'text',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "text_FFM = FFMModel(ratings_train, ratings_val, text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "text_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.18948085767652612  MSE:  1.778409672390595  MAE:  1.064447769604502\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = text_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with User and Business Information + Graph Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','graph_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'graph',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "graph_FFM = FFMModel(ratings_train, ratings_val, graph_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "graph_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.19572440525321522  MSE:  1.7647103224053686  MAE:  1.072474083328964\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = graph_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics All clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','graph_cluster','location_cluster','text_cluster'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'all_clusters',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "all_FFM = FFMModel(ratings_train, ratings_val, all_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "all_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.1862330235045122  MSE:  1.7855359441887817  MAE:  1.065033877918174\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = all_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The baseline with location cluster performs best as it has lowest MSE. Therefore, this model will be used. The following analysis is done on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFM runs in linear time which leads to fast runtime. Our model takes just two minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import time\n",
    "start_time = time.time()\n",
    "location_FFM = FFMModel(ratings_train, ratings_val, location_config)\n",
    "location_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 206.21558594703674 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "start_time = time.time()\n",
    "location_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.16828489303588867 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage and Serendipity metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = location_FFM.evaluate_on_val()\n",
    "predict_df['predictions'] = predictions['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011627906976744186 0.8576470588235293 0.9990697674418605 72.29767441860466\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df, out, ratings_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepctr[cpu]\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/46/2a3291f804c56ba0e5ba31d6c825c23bb1a2c7d8f6f066db28f8ed5f374a/deepctr-0.7.0-py3-none-any.whl (79kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 2.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting requests (from deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 10.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting h5py (from deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.9MB 418kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\" in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from deepctr[cpu])\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 8.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from requests->deepctr[cpu])\n",
      "Collecting idna<2.9,>=2.5 (from requests->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 10.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting six (from h5py->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.7 (from h5py->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/ab/43e678759326f728de861edbef34b8e2ad1b1490505f20e0d1f0716c3bf4/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.0MB 60kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: keras-preprocessing>=1.0.5 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: termcolor>=1.1.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: tensorboard<1.15.0,>=1.14.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: keras-applications>=1.0.6 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: protobuf>=3.6.1 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: google-pasta>=0.1.6 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: grpcio>=1.8.6 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: absl-py>=0.7.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: gast>=0.2.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: astor>=0.6.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Collecting wheel>=0.26 (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: wrapt>=1.11.1 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: setuptools>=41.0.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: markdown>=2.6.8 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Installing collected packages: urllib3, certifi, idna, requests, six, numpy, h5py, deepctr, wheel, werkzeug\n",
      "  Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "  Found existing installation: certifi 2018.1.18\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (certifi) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling certifi-2018.1.18:\n",
      "      Successfully uninstalled certifi-2018.1.18\n",
      "  Found existing installation: idna 2.6\n",
      "    Uninstalling idna-2.6:\n",
      "      Successfully uninstalled idna-2.6\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "  Found existing installation: h5py 2.7.1\n",
      "    Uninstalling h5py-2.7.1:\n",
      "      Successfully uninstalled h5py-2.7.1\n",
      "  Found existing installation: wheel 0.30.0\n",
      "    Uninstalling wheel-0.30.0:\n",
      "      Successfully uninstalled wheel-0.30.0\n",
      "  Found existing installation: Werkzeug 0.14.1\n",
      "    Uninstalling Werkzeug-0.14.1:\n",
      "      Successfully uninstalled Werkzeug-0.14.1\n",
      "Successfully installed certifi-2019.11.28 deepctr-0.7.0 h5py-2.10.0 idna-2.8 numpy-1.17.4 requests-2.22.0 six-1.13.0 urllib3-1.25.7 werkzeug-0.16.0 wheel-0.33.6\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U deepctr[cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr.models import DeepFM, CCPM, FNN, PNN, WDL, MLR, NFM, AFM, DCN, \\\n",
    "                            DIN, DIEN, DSIN, xDeepFM, AutoInt, NFFM, FGCNN, FiBiNET\n",
    "from deepctr.inputs import SparseFeat,get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = ratings_train_.copy()\n",
    "validation = ratings_cv_.copy()\n",
    "test = ratings_holdout_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>qdCwzhJ5Yo_Sdm_bYDIfOQ</td>\n",
       "      <td>2011-09-11 06:09:33</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I found Kathy's from yelp.  I love to support ...</td>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>XS1Zx6GzjtKPKmhDuVw5Jg</td>\n",
       "      <td>2017-06-19 22:55:06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I had the Saison infused with grapefruit which...</td>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>jLxeBgWhLRbII2ACkgH1Sg</td>\n",
       "      <td>2018-09-30 18:00:41</td>\n",
       "      <td>4.0</td>\n",
       "      <td>First time for me to come inside at least! Hav...</td>\n",
       "      <td>FIk4lQQu1eTe2EpzQ4xhBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>U_yacPCk8HgE1ywATmQUrg</td>\n",
       "      <td>2018-10-13 00:10:59</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Ordered for lunch with a few colleagues throug...</td>\n",
       "      <td>TpyOT5E16YASd7EWjLQlrw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>O-b5osM0NO4f31dp6_DatQ</td>\n",
       "      <td>2014-08-01 01:55:23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I can only comment on their macarons, which I'...</td>\n",
       "      <td>_N7Ndn29bpll_961oPeEfw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id                 date  rating  \\\n",
       "45   qdCwzhJ5Yo_Sdm_bYDIfOQ  2011-09-11 06:09:33     2.0   \n",
       "74   XS1Zx6GzjtKPKmhDuVw5Jg  2017-06-19 22:55:06     3.0   \n",
       "472  jLxeBgWhLRbII2ACkgH1Sg  2018-09-30 18:00:41     4.0   \n",
       "854  U_yacPCk8HgE1ywATmQUrg  2018-10-13 00:10:59     5.0   \n",
       "899  O-b5osM0NO4f31dp6_DatQ  2014-08-01 01:55:23     3.0   \n",
       "\n",
       "                                                  text                 user_id  \n",
       "45   I found Kathy's from yelp.  I love to support ...  d6xvYpyzcfbF_AZ8vMB7QA  \n",
       "74   I had the Saison infused with grapefruit which...  sG_h0dIzTKWa3Q6fmb4u-g  \n",
       "472  First time for me to come inside at least! Hav...  FIk4lQQu1eTe2EpzQ4xhBA  \n",
       "854  Ordered for lunch with a few colleagues throug...  TpyOT5E16YASd7EWjLQlrw  \n",
       "899  I can only comment on their macarons, which I'...  _N7Ndn29bpll_961oPeEfw  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert object to datetime\n",
    "# training.date = pd.to_datetime(training.date)\n",
    "# validation.date = pd.to_datetime(validation.date)\n",
    "# test.date = pd.to_datetime(test.date)\n",
    "\n",
    "# # find hour from datetime\n",
    "# training['hour'] = training.date.dt.hour\n",
    "# validation['hour'] = validation.date.dt.hour\n",
    "# test['hour'] = test.date.dt.hour\n",
    "\n",
    "test = test.loc[test.business_id.isin(training.business_id)]\n",
    "validation = validation.loc[validation.business_id.isin(training.business_id)]\n",
    "\n",
    "# map each user_id, business_id to an index\n",
    "# user_mapping = {}\n",
    "# for n,i in enumerate(training.user_id.unique()):\n",
    "#     user_mapping[i] = n\n",
    "\n",
    "# business_mapping = {}\n",
    "# for n,i in enumerate(training.business_id.unique()):\n",
    "#     business_mapping[i] = n\n",
    "\n",
    "# # for training\n",
    "# training['user_id'] = training['user_id'].map(user_mapping)\n",
    "# training['business_id'] = training['business_id'].map(business_mapping)\n",
    "# # for validation\n",
    "# validation['user_id'] = validation['user_id'].map(user_mapping)\n",
    "# validation['business_id'] = validation['business_id'].map(business_mapping)\n",
    "# # for test\n",
    "# test['user_id'] = test['user_id'].map(user_mapping)\n",
    "# test['business_id'] = test['business_id'].map(business_mapping)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-e1fea73c8687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hii!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y contains new labels: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36msetdiff1d\u001b[0;34m(ar1, ar2, assume_unique)\u001b[0m\n\u001b[1;32m    656\u001b[0m     array([[0, 2],\n\u001b[1;32m    657\u001b[0m            [4, 6]])\n\u001b[0;32m--> 658\u001b[0;31m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtest_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetxor1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "sparse_features = [\"user_id\", \"business_id\"]#, \"hour\"]\n",
    "target = ['rating']\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    training[feat] = lbe.fit_transform(training[feat])\n",
    "    print(\"hi!\")\n",
    "    validation[feat] = lbe.transform(validation[feat])\n",
    "    print(\"hii!\")\n",
    "    test[feat] = lbe.transform(test[feat])\n",
    "    print(\"hiii!\")\n",
    "\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, training[feat].nunique(), \\\n",
    "                                     embedding_dim = 8) for feat in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "print(\"hiiii!\")\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "train_model_input = {name:training[name].values for name in feature_names}\n",
    "valid_model_input = {name:validation[name].values for name in feature_names}\n",
    "print(\"hiiiii!\")\n",
    "test_model_input = {name:test[name].values for name in feature_names}\n",
    "\n",
    "# 4.Define Model,train,predict and evaluate\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression', \\\n",
    "            dnn_hidden_units = (128, 128), dnn_dropout = 0.3, l2_reg_embedding=1e-05, \\\n",
    "            l2_reg_dnn=1e-05, l2_reg_linear=1e-05)\n",
    "\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "\n",
    "history = model.fit(train_model_input, training[target].values, batch_size=256, \\\n",
    "                    epochs=10, verbose=2, validation_data= (valid_model_input, validation[target].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "\n",
    "print(\"test MSE\", round(mean_squared_error(\n",
    "        test[target].values, pred_ans), 4))\n",
    "\n",
    "pred_ans\n",
    "\n",
    "(test[target].values<2).sum(), (test[target].values<3).sum(), (test[target].values<4).sum()\n",
    "\n",
    "(pred_ans<2).sum(),(pred_ans<3).sum(), (pred_ans<4).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-323e9717061a>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-323e9717061a>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    **Caveat**: Even for a subsample (ratings_train is a subsample of full ratings dataset), there are restaurants rated by users but not **do not exist** in business profile dataset. This needs to be handled when building recommender system.\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Select restaurants in training set\n",
    "print(business.shape)\n",
    "train_business_id = np.unique(ratings_train.business_id)\n",
    "business = business.loc[business.business_id.isin(train_business_id), :]\n",
    "print(business.shape)\n",
    "\n",
    "### Business Profile\n",
    "\n",
    "tmp_series = pd.Series(business.category.str.split(', ').tolist(), index=business.business_id)\n",
    "business_df = pd.get_dummies(tmp_series.apply(pd.Series).stack()).sum(level=0).reset_index()\n",
    "\n",
    "business_df.head()\n",
    "\n",
    "business_df_normalized = business_df.set_index('business_id') \\\n",
    "                            .div(business_df.set_index('business_id').sum(axis=1)**0.5, axis=0)\n",
    "\n",
    "business_df_normalized.sort_values('business_id', inplace=True)\n",
    "\n",
    "business_df_normalized.head()\n",
    "\n",
    "### User Profile\n",
    "\n",
    "# check if all restaurants that users have rated exist in restuarant profile\n",
    "print('The number of all rated restaurants: {}'.format(len(np.unique(ratings_train[['business_id']]))))\n",
    "print('The number of resturants existing profile: {}'.format(len(np.unique(business_df.business_id))))\n",
    "\n",
    "# **Caveat**: Even for a subsample (ratings_train is a subsample of full ratings dataset), there are restaurants rated by users but not **do not exist** in business profile dataset. This needs to be handled when building recommender system. \n",
    "\n",
    "# Build uer profile\n",
    "users_id = np.unique(ratings_train.user_id)\n",
    "users_df = pd.DataFrame(columns = business_df_normalized.columns)\n",
    "\n",
    "approach_2_start = time.time()\n",
    "# create a for loop to create profile vector for each user\n",
    "for i in tqdm(range(len(users_id))):\n",
    "    selected_user_id = users_id[i]\n",
    "    # select business that user has rated\n",
    "    selected_rating = ratings_train.loc[ratings_train.user_id == selected_user_id, ['business_id', 'rating']]. \\\n",
    "                                        set_index('business_id')\n",
    "    ind = selected_rating.index\n",
    "    # To calculate user profile vector, \n",
    "    # multiply weight (normalized frequency) by ratings across all rated business and take mean of the result\n",
    "    try: \n",
    "        # only need to extract rated restaurants because the rest of term will be \n",
    "        w = business_df_normalized.loc[ind, :]\n",
    "    except KeyError:\n",
    "        business_id_existed_profile = business_df.business_id\n",
    "        filtered_ind = pd.Index([j for j in ind if j in business_id_existed_profile.tolist()])    \n",
    "        w = business_df_normalized.loc[filtered_ind, :]\n",
    "    except:\n",
    "        print('Some other errors occured during iteration i = {}'.format(i))\n",
    "    if (w.shape[0] == 0):\n",
    "        users_df.loc[selected_user_id] = 0\n",
    "    else:\n",
    "        profile_vec = w.mul(selected_rating.loc[:, 'rating'], axis=0).mean(axis=0)\n",
    "        users_df.loc[selected_user_id] = profile_vec\n",
    "        \n",
    "approach_2_duration = (time.time() - approach_2_start) * 1.0 / 60 \n",
    "print('The approach 2 takes {} minutes to run.'.format(approach_2_duration)) \n",
    "\n",
    "users_df.shape\n",
    "\n",
    "### CHECK\n",
    "# IDF vector  \n",
    "DF = business_df_normalized.sum(axis=0)\n",
    "IDF = (business_df.shape[0]/DF).apply(np.log) #log inverse of DF\n",
    "# The dot product of business profile vector and IDF vector gives\n",
    "# weighted score of each business\n",
    "IDF_business_df_normalized = business_df_normalized.mul(IDF.values)\n",
    "IDF_business_df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
